<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Mette Langaas">
<meta name="dcterms.date" content="2025-01-07">

<title>MA8701 Advanced methods in statistical inference and learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="P1W1_files/libs/clipboard/clipboard.min.js"></script>
<script src="P1W1_files/libs/quarto-html/quarto.js"></script>
<script src="P1W1_files/libs/quarto-html/popper.min.js"></script>
<script src="P1W1_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="P1W1_files/libs/quarto-html/anchor.min.js"></script>
<link href="P1W1_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="P1W1_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="P1W1_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="P1W1_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="P1W1_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul class="collapse">
  <li><a href="#messages-to-students" id="toc-messages-to-students" class="nav-link active" data-scroll-target="#messages-to-students">Messages to students</a>
  <ul class="collapse">
  <li><a href="#learning" id="toc-learning" class="nav-link" data-scroll-target="#learning">Learning</a></li>
  <li><a href="#plan-for-w1" id="toc-plan-for-w1" class="nav-link" data-scroll-target="#plan-for-w1">Plan for W1</a></li>
  </ul></li>
  <li><a href="#core-concepts" id="toc-core-concepts" class="nav-link" data-scroll-target="#core-concepts">Core concepts</a>
  <ul class="collapse">
  <li><a href="#notation" id="toc-notation" class="nav-link" data-scroll-target="#notation">Notation</a></li>
  <li><a href="#random-variables-and-random-vectors" id="toc-random-variables-and-random-vectors" class="nav-link" data-scroll-target="#random-variables-and-random-vectors">Random variables and random vectors</a></li>
  <li><a href="#training-set" id="toc-training-set" class="nav-link" data-scroll-target="#training-set">Training set</a></li>
  <li><a href="#validation-and-test-data" id="toc-validation-and-test-data" class="nav-link" data-scroll-target="#validation-and-test-data">Validation and test data</a></li>
  <li><a href="#group-discussion" id="toc-group-discussion" class="nav-link" data-scroll-target="#group-discussion">Group discussion</a></li>
  <li><a href="#regression-and-mlr" id="toc-regression-and-mlr" class="nav-link" data-scroll-target="#regression-and-mlr">Regression and MLR</a></li>
  </ul></li>
  <li><a href="#statistical-decision-theoretic-framework-for-regression" id="toc-statistical-decision-theoretic-framework-for-regression" class="nav-link" data-scroll-target="#statistical-decision-theoretic-framework-for-regression">Statistical decision theoretic framework for regression</a>
  <ul class="collapse">
  <li><a href="#squared-error-loss" id="toc-squared-error-loss" class="nav-link" data-scroll-target="#squared-error-loss">Squared error loss</a></li>
  <li><a href="#absolute-loss" id="toc-absolute-loss" class="nav-link" data-scroll-target="#absolute-loss">Absolute loss</a></li>
  <li><a href="#conclusions---and-next-step" id="toc-conclusions---and-next-step" class="nav-link" data-scroll-target="#conclusions---and-next-step">Conclusions - and next step</a></li>
  </ul></li>
  <li><a href="#statistical-decision-theoretic-framework-for-classification" id="toc-statistical-decision-theoretic-framework-for-classification" class="nav-link" data-scroll-target="#statistical-decision-theoretic-framework-for-classification">Statistical decision theoretic framework for classification</a>
  <ul class="collapse">
  <li><a href="#simple-example" id="toc-simple-example" class="nav-link" data-scroll-target="#simple-example">Simple example</a></li>
  <li><a href="#group-discussion-1" id="toc-group-discussion-1" class="nav-link" data-scroll-target="#group-discussion-1">Group discussion</a></li>
  </ul></li>
  <li><a href="#a-look-ahead" id="toc-a-look-ahead" class="nav-link" data-scroll-target="#a-look-ahead">A look ahead</a>
  <ul class="collapse">
  <li><a href="#week-2-working-with-epe" id="toc-week-2-working-with-epe" class="nav-link" data-scroll-target="#week-2-working-with-epe">Week 2: Working with EPE</a></li>
  <li><a href="#week-3-missing-data" id="toc-week-3-missing-data" class="nav-link" data-scroll-target="#week-3-missing-data">Week 3: Missing data</a></li>
  </ul></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises">Exercises</a></li>
  <li><a href="#solutions-to-exercises" id="toc-solutions-to-exercises" class="nav-link" data-scroll-target="#solutions-to-exercises">Solutions to exercises</a></li>
  <li><a href="#reference-links" id="toc-reference-links" class="nav-link" data-scroll-target="#reference-links">Reference links</a></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">Bibliography</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="P1W1.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li><li><a href="P1W1.pdf"><i class="bi bi-file-pdf"></i>Beamer</a></li></ul></div></nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">MA8701 Advanced methods in statistical inference and learning</h1>
<p class="subtitle lead">P1W1: Core concepts in statistical decision theory</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Mette Langaas </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 7, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="messages-to-students" class="level1">
<h1>Messages to students</h1>
<p>Course homepage: <a href="https://wiki.math.ntnu.no/ma8701/2025v/start" class="uri">https://wiki.math.ntnu.no/ma8701/2025v/start</a></p>
<ul>
<li><p>Read before the W1-lecture: ESL 2.2, 2.4, 3 (except 3.2.3, 3.2.4, 3.4, 3.7, 3.8), 4.1-4.5 (except 4.4.4). The only new topic (not taught in recommended courses) is 2.4.</p></li>
<li><p>Work on exercises in the bottom of this page after the W1-lecture.</p></li>
</ul>
<section id="learning" class="level2">
<h2 class="anchored" data-anchor-id="learning">Learning</h2>
<p>Herbert A. Simon (Cognitive science, Nobel Laureate): <em>Learning results from what the student does and thinks and only from what the student does and thinks. The teacher can advance learning only by influencing what the student does to learn.</em></p>
<hr>
</section>
<section id="plan-for-w1" class="level2">
<h2 class="anchored" data-anchor-id="plan-for-w1">Plan for W1</h2>
<ul>
<li>Notation (ESL Ch 2.2)</li>
<li>Regression - should be known from before (ESL Ch 3, except 3.2.3, 3.2.4, 3.4, 3.7, 3.8)</li>
<li>Statistical decision theoretic framework for regression (ESL 2.4)</li>
<li>Classification - should be known from before (ESL Ch 4.1-4.5, except 4.4.4)</li>
<li>Statistical decision theoretic framework for classification (ESL 2.4)</li>
</ul>
<hr>
</section>
</section>
<section id="core-concepts" class="level1">
<h1>Core concepts</h1>
<section id="notation" class="level2">
<h2 class="anchored" data-anchor-id="notation">Notation</h2>
<p>(mainly from ESL 2.2)</p>
<p>We will only consider supervised methods.</p>
<ul>
<li>Response <span class="math inline">\(Y\)</span> (or <span class="math inline">\(G\)</span>): dependent variable, outcome, usually univariate (but may be multivariate)
<ul>
<li>quantitative <span class="math inline">\(Y\)</span>: for regression</li>
<li>qualitative, categorical <span class="math inline">\(G\)</span>: for classification, some times dummy variable coding or one-hot coding used (what is the difference?)</li>
</ul></li>
<li>Covariates <span class="math inline">\(X_1, X_2, \ldots, X_p\)</span>: “independent variables”, predictors, features
<ul>
<li>continuous, discrete: used directly</li>
<li>categorical, discrete: often dummy variable coding or one-hot coding used (again, difference?)</li>
</ul></li>
</ul>
<p>We aim to construct a rule, function, learner: <span class="math inline">\(f(X)\)</span>, to predict <span class="math inline">\(Y\)</span> (or <span class="math inline">\(G\)</span>).</p>
<hr>
<p>Random variables and (column) vectors are written as uppercase letters <span class="math inline">\(X\)</span>, and <span class="math inline">\(Y\)</span>, while observed values are written with lowercase <span class="math inline">\((x,y)\)</span>. (Dimensions specified if needed.)</p>
<p>Matrices are presented with uppercase boldface: <span class="math inline">\(\boldsymbol{X}\)</span>, often <span class="math inline">\(N \times (p+1)\)</span>. ESL uses boldface also for <span class="math inline">\(\boldsymbol{x}_j\)</span> being a vector of all <span class="math inline">\(N\)</span> observations of variable <span class="math inline">\(j\)</span>, but in general vectors are not boldface and the vector of observed variables for observation <span class="math inline">\(i\)</span> is just <span class="math inline">\(x_i\)</span>.</p>
<hr>
</section>
<section id="random-variables-and-random-vectors" class="level2">
<h2 class="anchored" data-anchor-id="random-variables-and-random-vectors">Random variables and random vectors</h2>
<p>Both the response <em>and covariates</em> will be considered to be random, and drawn from some joint distribution <span class="math inline">\(P(X_1,X_2,\ldots, X_p,Y)=P(X,Y)\)</span> or <span class="math inline">\(P(X,G)\)</span>.</p>
<p>Joint to conditional and marginal distribution: <span class="math inline">\(P(X,Y)=P(Y \mid X)P(X)\)</span> or <span class="math inline">\(P(Y\mid X=x)P(X=x)\)</span> or</p>
<p><span class="math display">\[P(Y=y ,X=x)=P(Y=y\mid X=x)P(X=x)\]</span></p>
<!-- To look ahead: we may then calculate the expected value in a sequential (iterated) manner: -->
<!-- $$\text{E}[L(Y,f(X))]=\text{E}_{X,Y}[L(Y,f(X))]=\text{E}_{X}\text{E}_{Y \mid X}[L(Y,f(X))]$$ -->
<!-- where $L$ is a loss function (to be defined next) and $f(X)$ some function to predict $Y$ (or $G$). (No, $f(X)$ is not the density pdf.) -->
<p>Maybe brush up on this?</p>
<p><strong>Resources</strong></p>
<ul>
<li>From TMA4268: <a href="https://www.math.ntnu.no/emner/TMA4268/2019v/2StatLearn/2StatLearn.html#random_vector">Module 2 - Random vectors</a></li>
<li>From TMA4267: <a href="https://www.math.ntnu.no/emner/TMA4267/2017v/TMA4267V2017Part1.pdf">Part 1: Multivariate random variables and the multivariate normal distribution</a></li>
</ul>
<hr>
</section>
<section id="training-set" class="level2">
<h2 class="anchored" data-anchor-id="training-set">Training set</h2>
<p>(ESL 2.1)</p>
<p>A set of size <span class="math inline">\(N\)</span> of independent pairs of observations <span class="math inline">\((x_i,y_i)\)</span> is called the <em>training set</em> and often denoted <span class="math inline">\(\mathcal{T}\)</span>. Here <span class="math inline">\(x_i\)</span> may be a vector. Also <span class="math inline">\((X,Y)=\mathcal{T}\)</span> is used in articles.</p>
<p>The training data is used to estimate the unknown function <span class="math inline">\(f\)</span>.</p>
</section>
<section id="validation-and-test-data" class="level2">
<h2 class="anchored" data-anchor-id="validation-and-test-data">Validation and test data</h2>
<p>Validation data is used for <em>model selection</em> (finding the best model among a candidate set).</p>
<p>Test data is used for <em>model assessment</em> (assess the performance of the fitted model on future data).</p>
<p>We will consider theoretical results, and also look at different ways to split or resample available data.</p>
<p>More in ESL Chapter 7.</p>
<hr>
</section>
<section id="group-discussion" class="level2">
<h2 class="anchored" data-anchor-id="group-discussion">Group discussion</h2>
<p>Two core regression methods are multiple linear regression (MLR) and <span class="math inline">\(k\)</span>-nearest neighbour (kNN).</p>
<p>Choose one of the two methods:</p>
<ul>
<li>Set up the formal definition for <span class="math inline">\(f\)</span>, and model assumptions made</li>
<li>What top results do you remember? Write them down.</li>
<li>Are there challenges with the method?</li>
</ul>
<hr>
</section>
<section id="regression-and-mlr" class="level2">
<h2 class="anchored" data-anchor-id="regression-and-mlr">Regression and MLR</h2>
<p>See also the exercises!</p>
<p><strong>Resources</strong></p>
<p>(mostly what we learned in TMA4267, or ESL Ch 3, except 3.2.3, 3.2.4, 3.4, 3.7, 3.8)</p>
<ul>
<li>From TMA4268: <a href="https://www.math.ntnu.no/emner/TMA4268/2019v/TMA4268overview.html">Overview</a> and in particular <a href="https://www.math.ntnu.no/emner/TMA4268/2019v/3LinReg/3LinReg.html">Module 3: Linear regression</a></li>
<li>From TMA4315: <a href="https://www.math.ntnu.no/emner/TMA4315/2018h/TMA4315overviewH2018.html">Overview</a> and in particular <a href="https://www.math.ntnu.no/emner/TMA4315/2018h/2MLR.html">Module 2: MLR</a></li>
</ul>
<p>For <span class="math inline">\(k\)</span>NN see also Problem 1 of the <a href="https://www.math.ntnu.no/emner/TMA4268/Exam/V2018e.pdf">TMA4268 2018 exam</a> with <a href="https://www.math.ntnu.no/emner/TMA4268/Exam/e2018sol.pdf">solutions</a></p>
<hr>
</section>
</section>
<section id="statistical-decision-theoretic-framework-for-regression" class="level1">
<h1>Statistical decision theoretic framework for regression</h1>
<p>(ESL Ch 2.4, regression part)</p>
<p>is a mathematical framework for developing models <span class="math inline">\(f\)</span> - and assessing optimality.</p>
<hr>
<p>First, regression:</p>
<ul>
<li><span class="math inline">\(X \in \Re^p\)</span></li>
<li><span class="math inline">\(Y \in \Re\)</span></li>
<li><span class="math inline">\(P(X,Y)\)</span> joint distribution of covariates and response</li>
</ul>
<p>Aim: find a function <span class="math inline">\(f(X)\)</span> for predicting <span class="math inline">\(Y\)</span> from some inputs <span class="math inline">\(X\)</span>.</p>
<p>Ingredients: Loss function <span class="math inline">\(L(Y,f(X))\)</span> - for <em>penalizing errors in the prediction</em>.</p>
<p>Criterion for choosing <span class="math inline">\(f\)</span>: Expected prediction error (EPE) - later also to be referred to as Err.</p>
<hr>
<p><span class="math display">\[ \text{EPE}(f)=\text{E}_{X,Y}[L(Y,f(X))]=\int_{x,y}L(y,f(x))p(x,y)dxdy\]</span> Choose <span class="math inline">\(f\)</span> to minimize the <span class="math inline">\(\text{EPE}(f)\)</span>.</p>
<hr>
<p>Q: Why do we not involve the distribution of the random variable <span class="math inline">\(f(X)\)</span>, but instead the distribution of <span class="math inline">\(X\)</span>?</p>
<p>Law of the unconscious statistican (from our introductory course in statistics): <a href="https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician" class="uri">https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician</a> and <a href="https://tma4245.math.ntnu.no/forventing-og-varians/forventingsverdi/forventningsverdi-funksjoner-av-stokastiske-variabler-egx/">Thematics pages TMA4240/45</a>.</p>
<p>What is the most popular loss function for regression?</p>
<hr>
<section id="squared-error-loss" class="level2">
<h2 class="anchored" data-anchor-id="squared-error-loss">Squared error loss</h2>
<p><span class="math display">\[ \text{EPE}(f)=\text{E}_{X,Y}[L(Y,f(X))]=\text{E}_{X}\text{E}_{Y \mid X}[(Y-f(X))^2\mid X]\]</span></p>
<p>We want to minimize EPE, and see that it is sufficient to minimize <span class="math inline">\(\text{E}_{Y\mid X}[(Y-f(X))^2\mid X]\)</span> for each <span class="math inline">\(X=x\)</span> (pointwise):</p>
<p><span class="math display">\[ f(x)=\text{argmin}_c \text{E}_{Y \mid X}[(Y-c)^2 \mid X=x]\]</span> This gives as result the conditional expectation - the best prediction at any point <span class="math inline">\(X=x\)</span>:</p>
<p><span class="math display">\[ f(x)=\text{E}[Y \mid X=x]\]</span> Proof: by differentiating and setting equal 0. See also the exercises!</p>
<p>But, do we know this conditional distribution? In some cases only (which?). In practice: need to estimate <span class="math inline">\(f\)</span>.</p>
<hr>
<section id="what-if-the-joint-distribution-is-multivariate-normal" class="level3">
<h3 class="anchored" data-anchor-id="what-if-the-joint-distribution-is-multivariate-normal">What if the joint distribution is multivariate normal?</h3>
<p>Conditionally (known from before):</p>
<p>if we assume that <span class="math inline">\((X,Y) \sim N_{p+1}(\mu,\Sigma)\)</span> then we have seen (TMA4267) that <span class="math inline">\(\text{E}(Y\mid X)\)</span> is linear in <span class="math inline">\(X\)</span> and <span class="math inline">\(\text{Cov}(Y \mid X)\)</span> is independent of <span class="math inline">\(X\)</span>.</p>
<p><a href="https://www.math.ntnu.no/emner/TMA4268/2019v/2StatLearn/2StatLearn.html#six_useful_properties_of_the_mvn">Properties of the mvN</a></p>
<p>The conditional distributions of the components are (multivariate) normal. <span class="math display">\[\mathbf{X}_2 \mid (\mathbf{X}_1=\mathbf{x}_1) \sim N_{p2}(\mathbf{\mu}_2+\Sigma_{21}\Sigma_{11}^{-1} (\mathbf{x}_1-\mathbf{\mu}_1),\Sigma_{22}-\Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12}).\]</span></p>
<!-- Then we know we get  -->
<!-- $\hat{\beta}=(X^TX)^{-1}X^T Y$ (with matrices) using OLS or MLE. -->
<hr>
</section>
<section id="approximate-linear-model" class="level3">
<h3 class="anchored" data-anchor-id="approximate-linear-model">Approximate linear model</h3>
<p>But, also if we assume an approximate linear model: <span class="math inline">\(f(x)\approx x^T \beta\)</span></p>
<p>Marginally: <span class="math inline">\(\text{argmin}_{\beta} \text{E}[(Y-X^T\beta)^2]\)</span> gives <span class="math inline">\(\beta=\text{E}[X X^T]^{-1}\text{E}[XY]\)</span> (now random vectors).</p>
<p>We may replace expectations with averages in training data to estimate <span class="math inline">\(\beta\)</span>.</p>
<p>This is not conditional on <span class="math inline">\(X\)</span>, but we have assumed a linear relationship.</p>
<hr>
</section>
<section id="knn-and-conditional-expectation" class="level3">
<h3 class="anchored" data-anchor-id="knn-and-conditional-expectation">kNN and conditional expectation</h3>
<p>Local conditional mean for observations in <span class="math inline">\(\cal T\)</span> close to <span class="math inline">\({\mathbf x}_0\)</span>: <span class="math display">\[\hat{f}({\mathbf x}_0)=\frac{1}{k}\sum_{i \in \cal N_k({\mathbf x}_0)}Y_i\]</span></p>
<hr>
</section>
</section>
<section id="absolute-loss" class="level2">
<h2 class="anchored" data-anchor-id="absolute-loss">Absolute loss</h2>
<p>Regression with absolute (L1) loss: <span class="math inline">\(L(Y,f(X))=\lvert Y-f(X) \rvert\)</span> gives <span class="math inline">\(\hat{f}(x)=\text{median}(Y\mid X=x)\)</span>.</p>
<p>Proof: for example pages 8-11 of <a href="https://getd.libs.uga.edu/pdfs/ma_james_c_201412_ms.pdf" class="uri">https://getd.libs.uga.edu/pdfs/ma_james_c_201412_ms.pdf</a></p>
</section>
<section id="conclusions---and-next-step" class="level2">
<h2 class="anchored" data-anchor-id="conclusions---and-next-step">Conclusions - and next step</h2>
<ul>
<li>What are key take home messages so far?</li>
</ul>
<p>Continue with the decision theoretic framework for classification.</p>
<ul>
<li>Classification - should not be new (ESL Ch 4.1-4.5, except 4.4.4)</li>
<li>Statistical decision theoretic framework for classification (ESL 2.4) <!-- * and the bias-variance trade-off (ESL 2.9 and 7.2-7.3) --></li>
</ul>
<hr>
</section>
</section>
<section id="statistical-decision-theoretic-framework-for-classification" class="level1">
<h1>Statistical decision theoretic framework for classification</h1>
<p>(ESL Ch 2.4)</p>
<p><span class="math display">\[ \text{EPE}(f)=\text{E}_{X,Y}[L(Y,f(X))]=\int_{x,y}L(y,f(x))p(x,y)dxdy\]</span> Choose <span class="math inline">\(f\)</span> to minimize the <span class="math inline">\(\text{EPE}(f)\)</span>.</p>
<p>What changes do we need to do for classifiation?</p>
<hr>
<ul>
<li><span class="math inline">\(X \in \Re^p\)</span></li>
<li><span class="math inline">\(G \in {\cal G}=\{1,\ldots,K\}\)</span></li>
<li><span class="math inline">\(\hat{G}(X) \in {\cal G}=\{1,\ldots,K\}\)</span> (why <span class="math inline">\(f\)</span> for regression and <span class="math inline">\(\hat{G}\)</span> for classification? strange?)</li>
<li><span class="math inline">\(L(G,\hat{G}(X))\)</span> is a function with <span class="math inline">\(K\times K\)</span> possible values where <span class="math inline">\(K=\lvert G \rvert\)</span>. We may present this as a matrix with with elements <span class="math inline">\(l_{jk}\)</span> giving the price to pay to misclassify an observation with true class <span class="math inline">\(g_j\)</span> to class <span class="math inline">\(g_k\)</span>.</li>
<li>Elements on the diagonal of <span class="math inline">\(L\)</span> is 0, and off-diagonal elements are often <span class="math inline">\(1\)</span>.</li>
</ul>
<hr>
<p>We would like to find <span class="math inline">\(\hat{G}\)</span> to minimize the EPE:</p>
<p><span class="math display">\[\text{EPE}=\text{E}_{G,X}[L(G,\hat{G}(X))]=\text{E}_X \text{E}_{G\mid X}[L(G,\hat{G}(X))]\]</span> <span class="math display">\[=\text{E}_X \{ \sum_{k=1}^K L(g_k,\hat{G}(X))P(G=g_k \mid X=x) \} \]</span></p>
<hr>
<p>Also here it is sufficient to minimize the loss for each value of <span class="math inline">\(x\)</span> (pointwise)</p>
<p><span class="math display">\[ \hat{G}=\text{argmin}_{g \in {\cal G}}\sum_{k=1}^K L(g_k,\hat{G}(X))P(G=g_k \mid X=x) \]</span></p>
<p>In the special case of 0-1 loss (off-diagonal elements in <span class="math inline">\(L\)</span> equal to 1) then all <span class="math inline">\(k\)</span> except the correct class gives loss <span class="math inline">\(1\)</span> with probability <span class="math inline">\(P(G=g_k \mid X=x)\)</span>. Summing over the wrong classes gives the same as taking <span class="math inline">\(1\)</span> minus the conditional probability of the correct class <span class="math inline">\(g\)</span>.</p>
<hr>
<p><span class="math display">\[\hat{G}=\text{argmin}_{g \in {\cal G}} [1-P(G=g \mid X=x)]\]</span></p>
<p><span class="math display">\[=\text{argmax}_{g \in {\cal G}}P(G=g \mid X=x)\]</span></p>
<p>The <em>Bayes classifier</em> classifies to the most probable class using the conditional distribution <span class="math inline">\(P(G \mid X)\)</span>. The class boundaries are the <em>Bayes decision boundaries</em> and the error rate is the <em>Bayes rate</em>.</p>
<!-- Note: can also achieve the same result with dummy variable coding for classes and squared error. -->
<hr>
<section id="simple-example" class="level2">
<h2 class="anchored" data-anchor-id="simple-example">Simple example</h2>
<p>Let assume we have <span class="math inline">\(G=2\)</span>, <span class="math inline">\(p=1\)</span>, and know <span class="math inline">\(P(G=1 \mid X=x)=\frac{\exp(\beta_0+\beta_1 x)}{1+\exp(\beta_0+\beta_1 x)}\)</span> where <span class="math inline">\(\beta_0=0\)</span> and <span class="math inline">\(\beta_1=0.8\)</span>. The Bayes classifier classifies to group <span class="math inline">\(1\)</span> if <span class="math inline">\(x&gt;0\)</span>, as shown below.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="P1W1_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>In practice we don´t know that this is true or the value for the two parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, so we estimate from data.</p>
<hr>
</section>
<section id="group-discussion-1" class="level2">
<h2 class="anchored" data-anchor-id="group-discussion-1">Group discussion</h2>
<ol type="1">
<li>What do we know about classification (TMA4268 and TMA4315 mainly, or ESL ch 4.1-4.5, except 4.4.4):</li>
</ol>
<ul>
<li>What is the difference between discrimination and classification?</li>
<li>What are the sampling vs diagnostic paradigm? Give an example of one method of each type. How does this relate to what we have learned about the optimal solution for the 0/1-loss?</li>
<li>Give an example of one parametric and one non-parametric classification method.</li>
</ul>
<ol start="2" type="1">
<li><p>Logistic regression is by many seen as the “most important method in machine learning”. What do we remember about logistic regression? (Will be a very important method in Part 2.)</p></li>
<li><p>What “changes” need to be done to 2) when we have <span class="math inline">\(K&gt;2\)</span> classes?</p></li>
</ol>
<hr>
<p><strong>Resources</strong></p>
<p>(mostly what we learned in TMA4267, or ESL ch 4.1-4.5, except 4.4.4)</p>
<ul>
<li>From TMA4268: <a href="https://www.math.ntnu.no/emner/TMA4268/2019v/TMA4268overview.html">Overview</a> and in particular <a href="https://www.math.ntnu.no/emner/TMA4268/2019v/4Classif/4Classif.html">Module 4: Classification</a> and <a href="https://www.math.ntnu.no/emner/TMA4268/2019v/2StatLearn/2StatLearn.html#k-nearest_neighbour_classifier">Module 2: Statistical learning</a></li>
<li>From TMA4315: <a href="https://www.math.ntnu.no/emner/TMA4315/2018h/TMA4315overviewH2018.html">Overview</a> and in particular <a href="https://www.math.ntnu.no/emner/TMA4315/2018h/3BinReg.html">Module 3: Binary regression</a> and for more than two classes: <a href="https://www.math.ntnu.no/emner/TMA4315/2018h/6Categorical.html">Module 6: Categorical regression</a>.</li>
</ul>
</section>
</section>
<section id="a-look-ahead" class="level1">
<h1>A look ahead</h1>
<ul>
<li>But first: one word to describe important statistical theory discussed today!</li>
<li>Remember to work with the exercises!</li>
</ul>
<section id="week-2-working-with-epe" class="level2">
<h2 class="anchored" data-anchor-id="week-2-working-with-epe">Week 2: Working with EPE</h2>
<p>Cover new aspects for</p>
<ul>
<li>Model selection and assessment (ESL Ch 7.1-7.6, 7.10-7.12). Look through these parts of chapter 7 before the week 2!</li>
<li>Bias-variance trade-off is here - remind yourself of the derivation for the squared loss.</li>
<li>We will also look into overparameterized models - where the double descent becomes relevant! <span class="citation" data-cites="ISLR2">(<a href="#ref-ISLR2" role="doc-biblioref"><strong>ISLR2?</strong></a>)</span> Section 10.8.</li>
</ul>
</section>
<section id="week-3-missing-data" class="level2">
<h2 class="anchored" data-anchor-id="week-3-missing-data">Week 3: Missing data</h2>
<ul>
<li>How to handle missing data in data analyses. (Not in ESL)</li>
</ul>
<hr>
</section>
</section>
<section id="exercises" class="level1">
<h1>Exercises</h1>
<section id="law-of-total-expectation-and-total-variance" class="level3">
<h3 class="anchored" data-anchor-id="law-of-total-expectation-and-total-variance">1: Law of total expectation and total variance</h3>
<p>This is to get a feeling of the joint and conditional distributions, so that we understand expected value notation with joint, conditional and marginal distributions.</p>
<p>Give a derivation of the law of total expectation:</p>
<p><span class="math display">\[\text{E}[X]=\text{E}[\text{E}(X\mid Y)]\]</span></p>
<p>and the law of total variance: <span class="math display">\[\text{Var}[X]=\text{E}\text{Var}[X \mid Y]+\text{Var}\text{E}[X\mid Y]\]</span> (There is also a law of total covariance.)</p>
</section>
<section id="quadratic-loss-and-decision-theoretic-framework" class="level3">
<h3 class="anchored" data-anchor-id="quadratic-loss-and-decision-theoretic-framework">2: Quadratic loss and decision theoretic framework</h3>
<p>Show that <span class="math inline">\(f(x)=\text{E}[Y \mid X=x]\)</span> for the quadratic loss.</p>
</section>
<section id="curse-of-dimensionality" class="level3">
<h3 class="anchored" data-anchor-id="curse-of-dimensionality">3: Curse of dimensionality</h3>
<p>Read ELS pages 22-23 and then answer Exercise 2.3 - which is to “Derive equation (2.24).”</p>
<p>Important take home messages:</p>
<ul>
<li>All sample points are close to an edge of the sample.</li>
<li>If data are uniformly distributed in an hypercube in <span class="math inline">\(p\)</span> dimensions, we need to cover <span class="math inline">\(r^{1/p}\)</span> of the the range of each input variable to capture a fraction <span class="math inline">\(r\)</span> of the observations.</li>
</ul>
</section>
<section id="key-results-from-mlr" class="level3">
<h3 class="anchored" data-anchor-id="key-results-from-mlr">4: Key results from MLR</h3>
<p>(These results are known from TMA4267 and TMA4315, but useful to brush up on?)</p>
<p>Assume we have a data set with independent observation pairs <span class="math inline">\((y_i,{\mathbf x}_i)\)</span> for <span class="math inline">\(i=1,\ldots,N\)</span>.</p>
<p><span class="math display">\[{\mathbf Y=X \boldsymbol{\beta}}+{\mathbf \varepsilon}\]</span> where <span class="math inline">\({\mathbf Y}\)</span> is a <span class="math inline">\(N \times 1\)</span> random column vector, <span class="math inline">\({\mathbf X}\)</span> a <span class="math inline">\(N \times (p+1)\)</span> design matrix with row for observations (<span class="math inline">\({\mathbf x}^T_i\)</span>) and columns for covariates, and <span class="math inline">\({\mathbf \varepsilon}\)</span> <span class="math inline">\(N \times 1\)</span> random column vector</p>
<p>The assumptions for the classical linear model is:</p>
<ol type="1">
<li><p><span class="math inline">\(\text{E}({\mathbf \varepsilon})={\mathbf 0}\)</span>.</p></li>
<li><p><span class="math inline">\(\text{Cov}(\varepsilon)=\text{E}(\varepsilon \varepsilon^T)=\sigma^2{\mathbf I}\)</span>.</p></li>
<li><p>The design matrix has full rank, <span class="math inline">\(\text{rank}\)</span> <span class="math inline">\(({\mathbf X})=(p+1)\)</span>.</p></li>
</ol>
<p>The classical <em>normal</em> linear regression model is obtained if additionally</p>
<ol start="4" type="1">
<li><span class="math inline">\(\varepsilon\sim N_n({\mathbf 0},\sigma^2{\mathbf I})\)</span> holds.</li>
</ol>
<p>For random covariates these assumptions are to be understood conditionally on <span class="math inline">\({\mathbf X}\)</span>.</p>
<section id="a-regression-parameter-estimator" class="level4">
<h4 class="anchored" data-anchor-id="a-regression-parameter-estimator">a) Regression parameter estimator</h4>
<p>Derive the ordinary least squares (OLS) (or the maximum likelihood) estimator <span class="math inline">\(\hat{\beta}\)</span>.</p>
</section>
<section id="b-properties-of-regression-parameter-estimator" class="level4">
<h4 class="anchored" data-anchor-id="b-properties-of-regression-parameter-estimator">b) Properties of regression parameter estimator</h4>
<p>Derive the distribution of <span class="math inline">\(\hat{\beta}\)</span> (when assumption 4 is true).</p>
</section>
<section id="c-estimator-for-variance" class="level4">
<h4 class="anchored" data-anchor-id="c-estimator-for-variance">c) Estimator for variance</h4>
<p>Explain how we may find the restricted maximum likelihood estimator for <span class="math inline">\(\sigma^2\)</span>. Which distribution is used for inference for <span class="math inline">\(\sigma^2\)</span>?</p>
</section>
<section id="d-hypothesis-test" class="level4">
<h4 class="anchored" data-anchor-id="d-hypothesis-test">d) Hypothesis test</h4>
<p>How would you test the hypothesis <span class="math inline">\(H_0: \beta_j=0\)</span> against <span class="math inline">\(H_1: \beta_j\neq 0\)</span>?</p>
</section>
<section id="e-explanability" class="level4">
<h4 class="anchored" data-anchor-id="e-explanability">e) Explanability</h4>
<p>Explanability is now very important - but then we usually talk about black box models. How would you explain the impact of each covariate in a multiple linear regression model? Can you give the proportion of the varibility that each variable is responsible for explaining?</p>
</section>
</section>
<section id="key-results-from-the-statistical-decision-theoretic-framework" class="level3">
<h3 class="anchored" data-anchor-id="key-results-from-the-statistical-decision-theoretic-framework">5: Key results from the “Statistical decision theoretic framework”?</h3>
<ul>
<li>What are results to remember for regression and for classification?</li>
<li>How would you use these results?</li>
</ul>
</section>
<section id="derivation-of-bias-and-variance-of-estimators" class="level3">
<h3 class="anchored" data-anchor-id="derivation-of-bias-and-variance-of-estimators">6: Derivation of bias and variance of estimators</h3>
<ul>
<li><p>Derive the bias and variance for the <span class="math inline">\(k\)</span>NN estimator for regression, as given in Equation 7.10 on page 223 of ELS.</p></li>
<li><p>Derive the bias and variance for the OLS estimator, as given in Equation 7.11 on page 224 of ESL.</p></li>
</ul>
</section>
<section id="bayes-classier-bayes-decision-boundary-and-bayes-error-rate" class="level3">
<h3 class="anchored" data-anchor-id="bayes-classier-bayes-decision-boundary-and-bayes-error-rate">7: Bayes classier, Bayes decision boundary and Bayes error rate</h3>
<p>Solve TMA4268 exam problem 9 in 2019 at <a href="https://www.math.ntnu.no/emner/TMA4268/Exam/V2019e.pdf" class="uri">https://www.math.ntnu.no/emner/TMA4268/Exam/V2019e.pdf</a></p>
</section>
<section id="key-results-from-logistic-regression" class="level3">
<h3 class="anchored" data-anchor-id="key-results-from-logistic-regression">8: Key results from logistic regression</h3>
<section id="a-what-are-the-three-components-of-a-generalized-linear-model" class="level4">
<h4 class="anchored" data-anchor-id="a-what-are-the-three-components-of-a-generalized-linear-model">a) What are the three components of a generalized linear model?</h4>
</section>
<section id="b-what-are-these-three-for-a-logistic-regression" class="level4">
<h4 class="anchored" data-anchor-id="b-what-are-these-three-for-a-logistic-regression">b) What are these three for a logistic regression?</h4>
</section>
<section id="c-parameter-estimation" class="level4">
<h4 class="anchored" data-anchor-id="c-parameter-estimation">c) Parameter estimation</h4>
<p>How are regression parameters estimated for the GLM, and for logistic regression in particular?</p>
<p>Does it matter if you use the observed or expected information matrix for logistic regression?</p>
</section>
<section id="d-asymptotic-distribution" class="level4">
<h4 class="anchored" data-anchor-id="d-asymptotic-distribution">d) Asymptotic distribution</h4>
<p>What is the asymptotic distribution of the estimator for the regression parameter <span class="math inline">\(\hat{\beta}\)</span>? How can that be used to construct confidence intervals or perform hypothesis tests?</p>
</section>
<section id="e-deviance" class="level4">
<h4 class="anchored" data-anchor-id="e-deviance">e) Deviance</h4>
<p>How is the deviance defined in general, and how is this done for the logistic regression?</p>
</section>
</section>
</section>
<section id="solutions-to-exercises" class="level1">
<h1>Solutions to exercises</h1>
<p>Please try yourself first, or take a small peek - and try some more - before fully reading the solutions. Report errors or improvements to <a href="mailto:Mette.Langaas@ntnu.no" class="email">Mette.Langaas@ntnu.no</a>.</p>
<section id="law-of-total-e-and-var" class="level3">
<h3 class="anchored" data-anchor-id="law-of-total-e-and-var">1: Law of total E and Var</h3>
<p><a href="https://github.com/mettelang/MA8701V2023/blob/main/Part1/TotalEandTotalVar.pdf">Try first yourself</a></p>
</section>
<section id="quadratic-loss" class="level3">
<h3 class="anchored" data-anchor-id="quadratic-loss">2: Quadratic loss</h3>
<p>Page 8 of <a href="https://getd.libs.uga.edu/pdfs/ma_james_c_201412_ms.pdf" class="uri">https://getd.libs.uga.edu/pdfs/ma_james_c_201412_ms.pdf</a> or <a href="https://github.com/mettelang/MA8701V2023/blob/main/Part1/QuadLossCondEYmidx.pdf">Mettes notes</a></p>
</section>
<section id="curse-of-dimensionality-1" class="level3">
<h3 class="anchored" data-anchor-id="curse-of-dimensionality-1">3: Curse of dimensionality</h3>
<p><a href="https://github.com/mettelang/MA8701V2023/blob/main/Part1/ELSe23.pdf">2.3</a></p>
</section>
<section id="key-results-from-mlr-1" class="level3">
<h3 class="anchored" data-anchor-id="key-results-from-mlr-1">4: Key results from MLR</h3>
<section id="a-regression-parameter-estimator-1" class="level4">
<h4 class="anchored" data-anchor-id="a-regression-parameter-estimator-1">a) Regression parameter estimator</h4>
<p>Both methods are written out in <a href="https://www.math.ntnu.no/emner/TMA4268/2018v/notes/LeastSquaresMLR.pdf">these class notes from TMA4267/8</a>. More on likelihood-version here: <a href="https://www.math.ntnu.no/emner/TMA4315/2018h/2MLR.html#likelihood_theory_(from_b4)">TMA4315 GLM Module 2</a>.</p>
</section>
<section id="b-properties-of-regression-estimator" class="level4">
<h4 class="anchored" data-anchor-id="b-properties-of-regression-estimator">b) Properties of regression estimator</h4>
<p><a href="https://www.math.ntnu.no/emner/TMA4268/2019v/notes/M3L2notes.pdf">Page 3 of classnotes from TMA4268</a></p>
</section>
<section id="c-estimator-for-variance-1" class="level4">
<h4 class="anchored" data-anchor-id="c-estimator-for-variance-1">c) Estimator for variance</h4>
<p><a href="https://www.math.ntnu.no/emner/TMA4315/2018h/2MLR.html#restricted_maximum_likelihood_estimator_for_(%7Bbf_sigma%7D%5E2)">TMA4315 GLM Module 2</a></p>
</section>
<section id="d-hypothesis-test-1" class="level4">
<h4 class="anchored" data-anchor-id="d-hypothesis-test-1">d) Hypothesis test</h4>
<p><a href="https://www.math.ntnu.no/emner/TMA4268/2019v/notes/M3L2notes.pdf">Page 4 of classnotes from TMA4268</a></p>
</section>
<section id="e" class="level4">
<h4 class="anchored" data-anchor-id="e">e)</h4>
<p>Standardized regression coefficients, estimated regression coefficients divided by their standard errors, is the most used solution. But, also popular is the decomposition of the <span class="math inline">\(R^2\)</span> - easy for orthogonal design matrix, but not easy for correlated covariates. The LMG-method of <span class="citation" data-cites="gromping2007">Grömping (<a href="#ref-gromping2007" role="doc-biblioref">2007</a>)</span> (decomposing <span class="math inline">\(R^2\)</span>) gives a solution that is also valid with correlated covariates, that is identical to the Shapley value of Part 4 (more later).</p>
</section>
</section>
<section id="key-results-from-the-statistical-decision-theoretic-framework-1" class="level3">
<h3 class="anchored" data-anchor-id="key-results-from-the-statistical-decision-theoretic-framework-1">5: Key results from the “Statistical decision theoretic framework”?</h3>
<section id="what-are-results-to-remember-for-regression-and-for-classification" class="level4">
<h4 class="anchored" data-anchor-id="what-are-results-to-remember-for-regression-and-for-classification">What are results to remember for regression and for classification?</h4>
<ul>
<li>The EPE(f) is minimized.</li>
<li>Formula for EPE as function of loss function</li>
<li>For quadratic loss the optimal f is the conditinal expected value of E(Y X)</li>
<li>For 0/1-loss (classification) the optimal prediction is the class with the maximal posterior probability.</li>
</ul>
</section>
<section id="how-would-you-use-these-results" class="level4">
<h4 class="anchored" data-anchor-id="how-would-you-use-these-results">How would you use these results?</h4>
<ul>
<li>Open our eyes to why linear regression is so popular?</li>
<li>Understand why it is important to estimate class probabilities?</li>
</ul>
</section>
</section>
<section id="derivation-of-bias-and-variance-of-estimators-1" class="level3">
<h3 class="anchored" data-anchor-id="derivation-of-bias-and-variance-of-estimators-1">6: Derivation of bias and variance of estimators</h3>
<section id="k-nn" class="level4">
<h4 class="anchored" data-anchor-id="k-nn">k-NN:</h4>
<p><a href="https://waxworksmath.com/Authors/G_M/Hastie/WriteUp/Weatherwax_Epstein_Hastie_Solution_Manual.pdf">See page 104, and equation 130 of the solutions manual</a></p>
</section>
<section id="ols" class="level4">
<h4 class="anchored" data-anchor-id="ols">OLS:</h4>
<p>We solved this in 2019 in <a href="https://www.math.ntnu.no/emner/TMA4268/2019v/CompEx/Compulsory1withLF.html#problem_3:_bias-variance_trade-off">a compulsory exercise in TMA4268</a>, where we also compared with the ridge (to be studied in Part 2) and also plotted functions.</p>
</section>
</section>
<section id="bayes-classier-bayes-decision-boundary-and-bayes-error-rate-1" class="level3">
<h3 class="anchored" data-anchor-id="bayes-classier-bayes-decision-boundary-and-bayes-error-rate-1">7: Bayes classier, Bayes decision boundary and Bayes error rate</h3>
<p><a href="https://www.math.ntnu.no/emner/TMA4268/Exam/e2019sol.html" class="uri">https://www.math.ntnu.no/emner/TMA4268/Exam/e2019sol.html</a></p>
</section>
<section id="key-results-from-logistic-regression-1" class="level3">
<h3 class="anchored" data-anchor-id="key-results-from-logistic-regression-1">8: Key results from logistic regression</h3>
<section id="a-what-are-the-three-components-of-a-generalized-linear-model-1" class="level4">
<h4 class="anchored" data-anchor-id="a-what-are-the-three-components-of-a-generalized-linear-model-1">a) What are the three components of a generalized linear model?</h4>
<p><a href="https://www.math.ntnu.no/emner/TMA4315/2018h/5GLM.html">TMA4315 GLM, module 5</a></p>
</section>
<section id="b-what-are-these-three-for-a-logistic-regression-1" class="level4">
<h4 class="anchored" data-anchor-id="b-what-are-these-three-for-a-logistic-regression-1">b) What are these three for a logistic regression?</h4>
<p><a href="https://www.math.ntnu.no/emner/TMA4315/2018h/3BinReg.html#how_to_model_a_binary_response">Binary response from TMA4315 Module 3</a></p>
</section>
<section id="c-parameter-estimation-1" class="level4">
<h4 class="anchored" data-anchor-id="c-parameter-estimation-1">c) Parameter estimation</h4>
<p><a href="https://www.math.ntnu.no/emner/TMA4315/2018h/3BinReg.html#likelihood_and_derivations_thereof_-_continued">TMA4315 GLM Module 2</a>)</p>
</section>
<section id="d-asymptotic-distribution-1" class="level4">
<h4 class="anchored" data-anchor-id="d-asymptotic-distribution-1">d) Asymptotic distribution</h4>
<p><a href="https://www.math.ntnu.no/emner/TMA4315/2018h/5GLM.html#distribution_of_the_mle">TMA4315 GLM, module 5</a></p>
</section>
<section id="e-deviance-1" class="level4">
<h4 class="anchored" data-anchor-id="e-deviance-1">e) Deviance</h4>
<p><a href="https://www.math.ntnu.no/emner/TMA4315/2018h/3BinReg.html#grouped_vs_individual_data">Grouped vs individual data - and deviance</a></p>
</section>
</section>
</section>
<section id="reference-links" class="level1">
<h1>Reference links</h1>
<ul>
<li><p><a href="https://hastie.su.domains/ElemStatLearn/">ESL official errata:</a> and choose “Errata” in the left menu</p></li>
<li><p><a href="https://waxworksmath.com/Authors/G_M/Hastie/hastie.html">ESL solutions to exercises</a></p></li>
</ul>
<!-- * [Jarle Tufto´s lecture notes in TMA4315 GLM, 168 pages ](https://drive.google.com/file/d/1c6AOUQRKf2W2xWMS_hZU-SKa_IYCjZfG/view) -->
<ul>
<li><p><a href="https://www.uio.no/studier/emner/matnat/math/STK-IN4300/h20/exercises.html">ESL solutions from UiO</a></p></li>
<li><p><a href="https://www.youtube.com/playlist?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e">Introduction to statistical learning with R, Video playlist</a></p></li>
<li><p><a href="https://www.youtube.com/playlist?list=PLoROMvodv4rNHU1-iPeDRH-J0cL-CrIda">Introduction to statistical learning with Python, Exercises playlist</a></p></li>
</ul>
</section>
<section id="bibliography" class="level1">
<h1>Bibliography</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-gromping2007" class="csl-entry" role="listitem">
Grömping, U. 2007. <span>“Estimators of Relative Importance in Linear Regression Based on Variance Decomposition.”</span> <em>The American Statistician</em> 61: 139–47.
</div>
<div id="ref-ESL" class="csl-entry" role="listitem">
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. Vol. 2. Springer series in statistics New York. <a href="https://hastie.su.domains/ElemStatLearn">hastie.su.domains/ElemStatLearn</a>.
</div>
</div>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>